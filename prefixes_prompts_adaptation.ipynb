{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7866699,"sourceType":"datasetVersion","datasetId":4615357}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opacus\n!pip install datasets==2.15","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch._C import NoopLogger\nimport torch.nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\nfrom transformers import BertModel, BertPreTrainedModel, BertTokenizer\nfrom transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutput, Seq2SeqLMOutput\nimport copy\nfrom transformers import (AutoConfig, AutoTokenizer, DataCollatorWithPadding,\n    AdamW, AutoTokenizer, default_data_collator)\nimport numpy as np\nfrom datasets import load_dataset, load_metric\nfrom dataclasses import dataclass\nfrom datasets import load_metric\nfrom torch.utils.data import DataLoader\nfrom opacus import PrivacyEngine\nfrom tqdm import tqdm\nfrom datasets import load_from_disk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following few cells have been adapted from this github repository: https://github.com/THUDM/P-tuning-v2. These define the classes for prefixes and soft prompts.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass ModelArguments:\n    model_name_or_path: str\n    config_name: None\n    tokenizer_name: None\n    hidden_dropout_prob: float\n    pre_seq_len: int\n    prefix_projection: bool\n    prefix_hidden_size: None\n    prefix: bool\n    prompt: bool\n    model_revision: str","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PrefixEncoder(torch.nn.Module):\n    r'''\n    The torch.nn model to encode the prefix\n\n    Input shape: (batch-size, prefix-length)\n\n    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n    '''\n    def __init__(self, config):\n        super().__init__()\n        self.prefix_projection = config.prefix_projection\n        if self.prefix_projection:\n            # Use a two-layer MLP to encode the prefix\n            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n            self.trans = torch.nn.Sequential(\n                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),\n                torch.nn.Tanh(),\n                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size)\n            )\n        else:\n            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.trans(prefix_tokens)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertPrefixForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n        self.bert = BertModel(config)\n        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n\n        for param in self.bert.parameters():\n            param.requires_grad = False\n\n        self.pre_seq_len = config.pre_seq_len\n        self.n_layer = config.num_hidden_layers\n        self.n_head = config.num_attention_heads\n        self.n_embd = config.hidden_size // config.num_attention_heads\n\n        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n        self.prefix_encoder = PrefixEncoder(config)\n\n        bert_param = 0\n        for name, param in self.bert.named_parameters():\n            bert_param += param.numel()\n        all_param = 0\n        for name, param in self.named_parameters():\n            all_param += param.numel()\n        total_param = all_param - bert_param\n        print('total param is {}'.format(total_param)) # 9860105\n\n    def get_prompt(self, batch_size):\n        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device)\n        past_key_values = self.prefix_encoder(prefix_tokens)\n        # bsz, seqlen, _ = past_key_values.shape\n        past_key_values = past_key_values.view(\n            batch_size,\n            self.pre_seq_len,\n            self.n_layer * 2,\n            self.n_head,\n            self.n_embd\n        )\n        past_key_values = self.dropout(past_key_values)\n        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n        return past_key_values\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        batch_size = input_ids.shape[0]\n        past_key_values = self.get_prompt(batch_size=batch_size)\n        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)\n        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            past_key_values=past_key_values,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertPromptForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = BertModel(config)\n        self.embeddings = self.bert.embeddings\n        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        \n        self.pre_seq_len = config.pre_seq_len\n        self.n_layer = config.num_hidden_layers\n        self.n_head = config.num_attention_heads\n        self.n_embd = config.hidden_size // config.num_attention_heads\n\n        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n        self.prefix_encoder = torch.nn.Embedding(self.pre_seq_len, config.hidden_size)\n    \n    def get_prompt(self, batch_size):\n        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device)\n        prompts = self.prefix_encoder(prefix_tokens)\n        return prompts\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        batch_size = input_ids.shape[0]\n        raw_embedding = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n        )\n        prompts = self.get_prompt(batch_size=batch_size)\n        inputs_embeds = torch.cat((prompts, raw_embedding), dim=1)\n        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)\n        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n\n        outputs = self.bert(\n            # input_ids,\n            attention_mask=attention_mask,\n            # token_type_ids=token_type_ids,\n            # position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            # past_key_values=past_key_values,\n        )\n\n        # pooled_output = outputs[1]\n        sequence_output = outputs[0]\n        sequence_output = sequence_output[:, self.pre_seq_len:, :].contiguous()\n        first_token_tensor = sequence_output[:, 0]\n        pooled_output = self.bert.pooler.dense(first_token_tensor)\n        pooled_output = self.bert.pooler.activation(pooled_output)\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(model_args, config: AutoConfig, fix_bert: bool = False):\n    if model_args.prefix:\n        config.hidden_dropout_prob = model_args.hidden_dropout_prob\n        config.pre_seq_len = model_args.pre_seq_len\n        config.prefix_projection = model_args.prefix_projection\n        config.prefix_hidden_size = model_args.prefix_hidden_size\n\n        model_class = BertPrefixForSequenceClassification\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n            revision=model_args.model_revision,\n        )\n    elif model_args.prompt:\n        config.pre_seq_len = model_args.pre_seq_len\n        config.hidden_dropout_prob = model_args.hidden_dropout_prob\n        model_class = BertPromptForSequenceClassification\n        model = model_class.from_pretrained(\n            model_args.model_name_or_path,\n            config=config,\n            revision=model_args.model_revision,\n        )\n\n    bert_param = 0\n    if fix_bert:\n        config.model_type == \"bert\"\n        for param in model.bert.parameters():\n            param.requires_grad = False\n        for _, param in model.bert.named_parameters():\n            bert_param += param.numel()\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n    total_param = all_param - bert_param\n    print('***** total param is {} *****'.format(total_param))\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading tokenized datasets from datasets.ipynb","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/privacy-datasets2/Privacy_datasets/qnli\"\n\ntokenized_datasets = load_from_disk(dataset_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=1024, shuffle=True)\neval_dataloader = DataLoader(tokenized_datasets[\"validation_matched\"], batch_size=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_args = ModelArguments(\n    model_name_or_path=\"prajjwal1/bert-tiny\",\n    config_name=None,\n    tokenizer_name=None,\n    hidden_dropout_prob=0.1,\n    pre_seq_len=10,\n    prefix_projection=False,\n    prefix_hidden_size=None,\n    prefix=False,\n    prompt=True,\n    model_revision= \"main\"\n)\n\nconfig = AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        num_labels=3\n        )\n\nconfig.problem_type = \"single_label_classification\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_model(model_args, config, True)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.005)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Privacy Engine","metadata":{}},{"cell_type":"code","source":"model.train()\ntarget_epsilon = 3\ntarget_delta = 1.0 / len(tokenized_datasets[\"train\"])\nepochs = 25\nmax_grad_norm = 0.7\n\nprivacy_engine = PrivacyEngine()\nmodel, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_dataloader,\n    target_epsilon=target_epsilon,\n    target_delta=target_delta,\n    epochs=epochs,\n    max_grad_norm=max_grad_norm\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(model):    \n    model.eval()\n\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for batch in eval_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += (preds == labels).sum().item()\n            total_predictions += labels.size(0)\n\n    avg_loss = total_loss / len(eval_dataloader)\n    accuracy = correct_predictions / total_predictions\n    model.train()\n    return avg_loss, accuracy\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training with Privacy Engine","metadata":{}},{"cell_type":"code","source":"from opacus.utils.batch_memory_manager import BatchMemoryManager\n\nloss_fn = CrossEntropyLoss()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(1, epochs+1):\n    losses = []\n\n    with BatchMemoryManager(\n        data_loader=train_dataloader, \n        max_physical_batch_size=1024, \n        optimizer=optimizer\n    ) as memory_safe_data_loader:\n        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n            \n            #print(outputs)\n            loss = outputs.loss\n            loss.backward()\n            losses.append(loss.item())\n\n            optimizer.step()\n\n    train_loss = np.mean(losses)\n    eps = privacy_engine.get_epsilon(target_delta) if privacy_engine else None\n\n    eval_loss, eval_accuracy = evaluate(model)\n    \n    print(\n        f\"Epoch: {epoch} | \"\n        f\"Train loss: {train_loss:.3f} | \"\n        f\"Eval loss: {eval_loss:.3f} | \"\n        f\"Eval accuracy: {eval_accuracy:.3f} | \"\n        f\"ɛ: {eps:.2f}\" if eps is not None else \"\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training without Privacy Engine","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_train_loss = 0\n    \n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n    \n    train_loss = total_train_loss / len(train_dataloader)\n    \n    model.eval()\n    with torch.no_grad():\n        eval_loss, eval_accuracy = evaluate(model)\n    \n    print(\n        f\"Epoch: {epoch} | \"\n        f\"Train loss: {train_loss:.3f} | \"\n        f\"Eval loss: {eval_loss:.3f} | \"\n        f\"Eval accuracy: {eval_accuracy:.3f} | \"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Performance on Test Set","metadata":{}},{"cell_type":"code","source":"def test(model, test_dataloader):\n    model.eval()\n\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n            loss = outputs.loss\n            total_loss += loss.item()\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += (preds == labels).sum().item()\n            total_predictions += labels.size(0)\n\n    avg_loss = total_loss / len(test_dataloader)\n    accuracy = correct_predictions / total_predictions\n\n    return avg_loss, accuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = test(model, test_dataloader)\n\nprint(f\"Accuracy: {test_accuracy:.3f}\")","metadata":{},"execution_count":null,"outputs":[]}]}