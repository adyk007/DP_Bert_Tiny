{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7866699,"sourceType":"datasetVersion","datasetId":4615357}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opacus\n!pip install datasets==2.15","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport numpy as np\nimport os\nfrom opacus import PrivacyEngine\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom datasets import load_from_disk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading tokenised datasets generated in datasets.ipynb","metadata":{}},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/privacy-datasets2/Privacy_datasets/mnli\"\n\ntokenized_datasets = load_from_disk(dataset_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=1024, shuffle=True)\neval_dataloader = DataLoader(tokenized_datasets[\"validation_matched\"], batch_size=1024)\ntest_dataloader = DataLoader(tokenized_datasets[\"test_matched\"], batch_size=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\",num_labels=3)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.005)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Freezing all layers of the model except for the Classification Layer","metadata":{}},{"cell_type":"code","source":"# Freeze all BERT layers\nfor param in model.bert.parameters():\n    param.requires_grad = False\n\n# Unfreeze the classification layer\nfor param in model.classifier.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trainable Parameters","metadata":{}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nnon_trainable_params = total_params - trainable_params\n\nprint(trainable_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Privacy Engine Setup","metadata":{}},{"cell_type":"code","source":"model.train()\ntarget_epsilon = 3\ntarget_delta = 1.0 / len(tokenized_datasets[\"train\"])\nepochs = 20\nmax_grad_norm = 0.7\n\nprivacy_engine = PrivacyEngine()\nmodel, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n    module=model,\n    optimizer=optimizer,\n   data_loader=train_dataloader,\n   target_epsilon=target_epsilon,\n    target_delta=target_delta,\n   epochs=epochs,\n   max_grad_norm=max_grad_norm\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(model):    \n    model.eval()\n\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():  \n        for batch in eval_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n            loss = outputs.loss\n            total_loss += loss.item()\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += (preds == labels).sum().item()\n            total_predictions += labels.size(0)\n\n    avg_loss = total_loss / len(eval_dataloader)\n    accuracy = correct_predictions / total_predictions\n    model.train()\n    return avg_loss, accuracy\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training with Privacy Engine","metadata":{}},{"cell_type":"code","source":"from opacus.utils.batch_memory_manager import BatchMemoryManager\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(1, epochs+1):\n    losses = []\n\n    with BatchMemoryManager(\n        data_loader=train_dataloader, \n        max_physical_batch_size=1024, \n        optimizer=optimizer\n    ) as memory_safe_data_loader:\n        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n            loss = outputs.loss\n            loss.backward()\n            losses.append(loss.item())\n\n            optimizer.step()\n\n    train_loss = np.mean(losses)\n    eps = privacy_engine.get_epsilon(target_delta) if privacy_engine else None\n\n    eval_loss, eval_accuracy = evaluate(model) \n\n    print(\n        f\"Epoch: {epoch} | \"\n        f\"Train loss: {train_loss:.3f} | \"\n        f\"Eval loss: {eval_loss:.3f} | \"\n        f\"Eval accuracy: {eval_accuracy:.3f} | \"\n        f\"É›: {eps:.2f}\" if eps is not None else \"\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training without Privacy Engine","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_train_loss = 0\n    \n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n    \n    train_loss = total_train_loss / len(train_dataloader)\n    \n    model.eval()\n    with torch.no_grad():\n        eval_loss, eval_accuracy = evaluate(model)\n    \n    print(\n        f\"Epoch: {epoch} | \"\n        f\"Train loss: {train_loss:.3f} | \"\n        f\"Eval loss: {eval_loss:.3f} | \"\n        f\"Eval accuracy: {eval_accuracy:.3f} | \"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Performance on Test Set","metadata":{}},{"cell_type":"code","source":"def test(model, test_dataloader):\n    model.eval()\n\n    total_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, token_type_ids=token_type_ids)\n            loss = outputs.loss\n            total_loss += loss.item()\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += (preds == labels).sum().item()\n            total_predictions += labels.size(0)\n\n    avg_loss = total_loss / len(test_dataloader)\n    accuracy = correct_predictions / total_predictions\n\n    return avg_loss, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = test(model, test_dataloader)\n\nprint(f\"Accuracy: {test_accuracy:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}