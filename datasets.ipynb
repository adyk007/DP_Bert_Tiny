{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nfrom opacus import PrivacyEngine\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer\nfrom datasets import load_from_disk","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n\n# Tokenize function\ndef tokenize_function(examples):\n    # Depending on the dataset, we might have one or two text fields\n    text_fields = ['sentence'] if 'sentence' in examples else ['question', 'sentence'] if 'question' in examples else ['premise', 'hypothesis'] if 'premise' in examples and 'hypothesis' in examples else ['question1', 'question2']\n    return tokenizer(*[examples[field] for field in text_fields], padding='max_length', truncation=True, max_length=128, return_token_type_ids=True)\n\n# List of tuples describing the dataset name and its type/name in the `datasets` library\ndataset_info = [\n    ('sst2', 'glue', 'sst2'),\n    ('qnli', 'glue', 'qnli'),\n    ('mnli', 'glue', 'mnli'),\n    ('qqp', 'glue', 'qqp'),\n]\n\n# Load and tokenize the datasets in a loop\ntokenized_datasets = {}\nfor dataset_name, library_name, dataset_type in dataset_info:\n    # Load the dataset\n    dataset = load_dataset(library_name, dataset_type)\n    \n    # Tokenize the dataset\n    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n    \n    # Rename the label column to 'labels'\n    if \"label\" in tokenized_dataset[\"train\"].column_names:\n        tokenized_dataset = tokenized_dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n        tokenized_dataset = tokenized_dataset.remove_columns(\"label\")\n    \n    # Set the format to PyTorch tensors\n    tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"labels\"])\n    \n    # Store the tokenized dataset\n    tokenized_datasets[dataset_name] = tokenized_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"/kaggle/working/tokenized_datasets\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Save each tokenized dataset\nfor dataset_name, tokenized_dataset in tokenized_datasets.items():\n    # Save the dataset to a file\n    save_path = os.path.join(output_dir, f\"{dataset_name}.dataset\")\n    tokenized_dataset.save_to_disk(save_path)","metadata":{},"execution_count":null,"outputs":[]}]}